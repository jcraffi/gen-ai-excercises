{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa5db227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, TaskType, PeftModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 96000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the AG News dataset\n",
    "# See: https://huggingface.co/datasets/fancyzhx/ag_news\n",
    "dataset = load_dataset(\"fancyzhx/ag_news\", split=\"train\").train_test_split(test_size=0.2, shuffle=True, seed=23)\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krist\\.conda\\envs\\tensor\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 24000/24000 [00:02<00:00, 8335.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 96000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the dataset using the RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the foundation model\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"albert-base-v2\",\n",
    "    num_labels=4,\n",
    "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
    "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
    ")\n",
    "\n",
    "foundation_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for param in foundation_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f7fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Prepare the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_train=False,\n",
    "    disable_tqdm=True,  # Disable progress bar\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=foundation_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a08003f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4078623056411743, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.2693333333333333, 'eval_runtime': 79.5406, 'eval_samples_per_second': 301.733, 'eval_steps_per_second': 18.858}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4078623056411743,\n",
       " 'eval_model_preparation_time': 0.001,\n",
       " 'eval_accuracy': 0.2693333333333333,\n",
       " 'eval_runtime': 79.5406,\n",
       " 'eval_samples_per_second': 301.733,\n",
       " 'eval_steps_per_second': 18.858}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 58,372 || all params: 11,745,032 || trainable%: 0.4970\n"
     ]
    }
   ],
   "source": [
    "# Configure the PEFT model\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "    \"albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query\",\n",
    "    \"albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key\",\n",
    "    \"albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value\",\n",
    "    \"albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense\",\n",
    "    \"albert.encoder.albert_layer_groups.0.albert_layers.0.ffn\",\n",
    "    \"albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output\"\n",
    "],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=4,\n",
    "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
    "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
    ")\n",
    "\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Initialize the PEFT model\n",
    "peft_model = PeftModelForSequenceClassification(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 500/1500 [05:49<14:55,  1.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9998, 'grad_norm': 13.405620574951172, 'learning_rate': 1.4844444444444445e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [14:40<04:30,  1.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4289, 'grad_norm': 16.54842185974121, 'learning_rate': 7.451851851851852e-06, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [22:12<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.377, 'grad_norm': 22.814571380615234, 'learning_rate': 4.444444444444445e-08, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1500/1500 [23:58<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.371499240398407, 'eval_accuracy': 0.8874583333333333, 'eval_runtime': 106.3049, 'eval_samples_per_second': 225.766, 'eval_steps_per_second': 14.11, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [23:58<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1438.6459, 'train_samples_per_second': 66.729, 'train_steps_per_second': 1.043, 'train_loss': 0.6018951110839844, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.6018951110839844, metrics={'train_runtime': 1438.6459, 'train_samples_per_second': 66.729, 'train_steps_per_second': 1.043, 'total_flos': 467276400806400.0, 'train_loss': 0.6018951110839844, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Define the training arguments\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./data/results/peft\",\n",
    "    # Set the learning rate\n",
    "    learning_rate=2e-5,\n",
    "    # Set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # Evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=500, \n",
    ")\n",
    "\n",
    "# Initialize the optimizer and scheduler\n",
    "optimizer = AdamW(peft_model.parameters(), lr=2e-5)\n",
    "total_steps = len(tokenized_dataset[\"train\"]) // peft_training_args.per_device_train_batch_size * peft_training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"/models/albert-base-v2-peft-ag-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "ag_news_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"/models/albert-base-v2-peft-ag-news\",\n",
    "    num_labels=4,\n",
    "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
    "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
    ")\n",
    "\n",
    "ag_news_model.config.pad_token_id = ag_news_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.371499240398407, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.8874583333333333, 'eval_runtime': 106.6825, 'eval_samples_per_second': 224.967, 'eval_steps_per_second': 14.06}\n",
      "{'eval_loss': 0.371499240398407, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.8874583333333333, 'eval_runtime': 106.6825, 'eval_samples_per_second': 224.967, 'eval_steps_per_second': 14.06}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the PEFT model\n",
    "peft_trainer = Trainer(\n",
    "    model=ag_news_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_evaluation_results = peft_trainer.evaluate()\n",
    "display(peft_evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 items from the test dataset\n",
    "test_samples = tokenized_dataset[\"test\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height: 400px; overflow-y: scroll;\"><table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Text</th>\n",
       "      <th>True Label</th>\n",
       "      <th>Foundation Model Prediction</th>\n",
       "      <th>PEFT Model Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Foulke, Wakefield are happy to pitch in As much as the Red Sox relished thumping the Yankees, they took special satisfaction from the outings of two key pitchers who recently have struggled, Keith Foulke and Tim Wakefield .</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nfld. oil spill more serious than originally thought ST. JOHN #39;S, Nfld. -- An oil spill on Newfoundland #39;s Grand Banks is larger than first reported. The Canada-Newfoundland Offshore Petroleum Board now says that up to 1,000 barrels of oil may have spilled into</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Shell to scrap twin board structure Oil giant Royal Dutch/Shell today said it would to scrap its twin board structure as it battles to restore confidence in the wake of its reserves crisis.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Death toll in Asian quake disaster passes 144,000 (AFP) AFP - The number of people killed in the massive earthquake and tidal waves that hit Indian Ocean shorelines a week ago passed 144,000.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NASA and Russia OK Next Space Station Crew In a separate development, the head of the RKK Energia company, which builds the Soyuz spacecraft, said his company was planning to send a new space shuttle to the space station between 2010 and 2012, depending on funding.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sony To Sell MP3 Music Players in Japan Sony #39;s two MP3 players, the NW-HD3 with a 20-gigabyte hard-disk drive, and the NW-E99, with a built-in 1-gigabyte flash-memory chip, go on sale December 10 in Japan.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dolphins Top Rams for 1st Win of Season (AP) AP - That noise the crowd made Sunday during the Miami Dolphins' victory at Pro Player Stadium was faintly familiar from seasons past. It's called cheering.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Second Sunni Cleric Gunned Down in Iraq (Reuters) Reuters - Gunmen killed a Sunni Muslim\\cleric in the city of Miqdadiya Tuesday, the second such\\killing in Iraq in as many days, witnesses and hospital\\officials said.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Trump #39;s Casino Operation Files for Bankruptcy Donald J. Trump #39;s struggling casino operation has filed for bankruptcy reorganization, according to court documents, effectively commencing a recapitalization plan that was announced last month.</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Lampard strikes as England cruise past Wales MANCHESTER: A stunning goal by captain David Beckham guided England to a convincing 2-0 victory over Wales in their World Cup qualifier at his former Old Trafford home yesterday.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Function to make predictions\n",
    "# Select 10 items from the test dataset\n",
    "test_samples = tokenized_dataset[\"test\"].select(range(10))\n",
    "\n",
    "def make_predictions(model, tokenizer, samples, device):\n",
    "    model.to(device)\n",
    "    inputs = tokenizer(samples[\"text\"], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make predictions using foundation_model\n",
    "foundation_predictions = make_predictions(foundation_model, tokenizer, test_samples, device)\n",
    "\n",
    "# Make predictions using peft_model\n",
    "peft_predictions = make_predictions(peft_model, tokenizer, test_samples, device)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df = pd.DataFrame({\n",
    "    \"Text\": test_samples[\"text\"],\n",
    "    \"True Label\": test_samples[\"label\"],\n",
    "    \"Foundation Model Prediction\": foundation_predictions,\n",
    "    \"PEFT Model Prediction\": peft_predictions\n",
    "})\n",
    "\n",
    "# Display the DataFrame as a scrollable element\n",
    "html = df.to_html(classes='table table-striped', index=False)\n",
    "display(HTML(f'<div style=\"max-height: 400px; overflow-y: scroll;\">{html}</div>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
